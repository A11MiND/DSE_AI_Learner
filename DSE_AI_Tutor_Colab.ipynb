{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac0ef5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Install Dependencies\n",
    "!pip install streamlit pyngrok requests pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b1f746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Install & Start Ollama (Local LLM Backend)\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "# Start Ollama in the background\n",
    "subprocess.Popen([\"ollama\", \"serve\"])\n",
    "time.sleep(5) # Wait for it to start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555b20fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Pull Vision Model (This may take a few minutes)\n",
    "# We use 'llava' as it is lightweight and reliable on Colab. \n",
    "# You can change this to 'qwen2.5-vl' if you want better performance but slower download.\n",
    "!ollama pull llava"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8c40e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create the App File (app.py)\n",
    "app_code = \"\"\"\n",
    "import streamlit as st\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Config ---\n",
    "OLLAMA_URL = 'http://localhost:11434'\n",
    "DEFAULT_MODEL = 'llava'\n",
    "\n",
    "st.set_page_config(page_title='DSE AI Tutor (Colab)', page_icon='üéì', layout='wide')\n",
    "\n",
    "def call_ollama_vision(model, image_bytes, prompt):\n",
    "    url = f'{OLLAMA_URL}/api/generate'\n",
    "    import base64\n",
    "    img_b64 = base64.b64encode(image_bytes).decode('utf-8')\n",
    "    payload = {\n",
    "        'model': model,\n",
    "        'prompt': prompt,\n",
    "        'images': [img_b64],\n",
    "        'stream': False\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(url, json=payload, timeout=180)\n",
    "        return response.json().get('response', '')\n",
    "    except Exception as e:\n",
    "        return f'Error: {str(e)}'\n",
    "\n",
    "# --- UI ---\n",
    "st.title('üéì DSE AI Tutor (Cloud Version)')\n",
    "st.info('Running on Google Colab GPU')\n",
    "\n",
    "if 'messages' not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "for msg in st.session_state.messages:\n",
    "    with st.chat_message(msg['role']):\n",
    "        st.markdown(msg['content'])\n",
    "        if 'image' in msg:\n",
    "            st.image(msg['image'], width=300)\n",
    "\n",
    "# Input\n",
    "with st.popover('üìé Attach Image'):\n",
    "    uploaded_file = st.file_uploader('Upload', type=['jpg','png'])\n",
    "\n",
    "if uploaded_file:\n",
    "    st.image(uploaded_file, width=150, caption='Preview')\n",
    "\n",
    "user_input = st.chat_input('Ask a question...')\n",
    "\n",
    "if user_input:\n",
    "    # User Message\n",
    "    with st.chat_message('user'):\n",
    "        st.markdown(user_input)\n",
    "        if uploaded_file:\n",
    "            st.image(uploaded_file, width=300)\n",
    "    \n",
    "    msg_data = {'role': 'user', 'content': user_input}\n",
    "    \n",
    "    # Logic\n",
    "    response_text = ''\n",
    "    if uploaded_file:\n",
    "        with st.spinner('üëÄ VLM Analyzing...'):\n",
    "            img_bytes = uploaded_file.getvalue()\n",
    "            prompt = f'You are a DSE Computer Science Tutor. {user_input}'\n",
    "            response_text = call_ollama_vision(DEFAULT_MODEL, img_bytes, prompt)\n",
    "            msg_data['image'] = img_bytes\n",
    "    else:\n",
    "        # Simple text fallback since AnythingLLM is not in Colab\n",
    "        response_text = '‚ö†Ô∏è Text-only RAG is not available in this Colab demo (requires local AnythingLLM). Please upload an image for VLM analysis.'\n",
    "    \n",
    "    st.session_state.messages.append(msg_data)\n",
    "    st.session_state.messages.append({'role': 'assistant', 'content': response_text})\n",
    "    \n",
    "    with st.chat_message('assistant'):\n",
    "        st.markdown(response_text)\n",
    "\"\"\"\n",
    "\n",
    "with open(\"app.py\", \"w\") as f:\n",
    "    f.write(app_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9482c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Run Streamlit with Ngrok Tunnel\n",
    "from pyngrok import ngrok\n",
    "\n",
    "# Terminate open tunnels if any\n",
    "ngrok.kill()\n",
    "\n",
    "# Set your authtoken (Optional but recommended for stability)\n",
    "# ngrok.set_auth_token(\"YOUR_NGROK_TOKEN\") \n",
    "\n",
    "# Open a HTTP tunnel on port 8501\n",
    "public_url = ngrok.connect(8501).public_url\n",
    "print(f\"üöÄ App is live at: {public_url}\")\n",
    "\n",
    "# Run Streamlit\n",
    "!streamlit run app.py"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
